
# üìù Publications

My full paper list can be found at <a href='https://scholar.google.com/citations?user=5RF4ia8AAAAJ'><img src="https://img.shields.io/endpoint?logo=Google%20Scholar&url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2FKyanChen%2FKyanChen.github.io@google-scholar-stats%2Fgs_data_shieldsio.json&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>.


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/2024_RSMamba.png"><img src='image/2024_RSMamba.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>RSMamba: Remote Sensing Image Classification with State Space Model</b><br>
<i>GRSL, 2024</i><br>
<b>Keyan Chen</b>, Bowen Chen, Chenyang Liu, Wenyuan Li, Zhengxia Zou, and Zhenwei Shi<br>
<b><font color="red">Popular Article of GRSL</font></b><br>
[<a href="https://arxiv.org/abs/2403.19654">Arxiv</a>] [<a href="https://github.com/KyanChen/RSMamba">Github</a>] <br>
<div style="text-align: justify">
We introduce RSMamba, a novel architecture for remote sensing image classification. RSMamba is based on the State Space Model (SSM) and incorporates an efficient, hardware-aware design known as the Mamba. To overcome the limitation of the vanilla Mamba, which can only model causal sequences and is not adaptable to two-dimensional image data, we propose a dynamic multi-path activation mechanism to augment Mamba's capacity to model non-causal data.
</div>
</div>

</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/2024_TTP.png"><img src='image/2024_TTP.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>Time Travelling Pixels: Bitemporal Features Integration with Foundation Model for Remote Sensing Image Change Detection</b><br>
<i>IGARSS, 2024</i><br>
<b>Keyan Chen</b>, Chenyang Liu, Wenyuan Li, Zili Liu, Hao Chen, Haotian Zhang, Zhengxia Zou, and Zhenwei Shi<br>
[<a href="https://arxiv.org/abs/2312.16202">Arxiv</a>] [<a href="https://github.com/KyanChen/TTP">Github</a>] [<a href="https://kyanchen.github.io/TTP/">Page</a>] [<a href="https://huggingface.co/spaces/KyanChen/TTP">Demo</a>]<br>
<div style="text-align: justify">
We integrate the latent knowledge of the SAM foundation model into change detection, effectively addressing the domain shift in general knowledge transfer and the challenge of expressing homogeneous and heterogeneous characteristics of multi-temporal images.
</div>
</div>

</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/2024_RSPrompter.png"><img src='image/2024_RSPrompter.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation based on Visual Foundation Model</b><br>
<i>IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2024</i><br>
<b>Keyan Chen</b>, Chenyang Liu, Hao Chen, Haotian Zhang, Wenyuan Li, Zhengxia Zou, and Zhenwei Shi<br>
üèÜÔ∏è <b><font color="red">ESI Highly Cited Paper & ESI Hot Paper</font></b><br>
<b><font color="red">Popular Article of TGRS</font></b><br>
[<a href="https://arxiv.org/abs/2306.16269">Arxiv</a>] [<a href="https://github.com/KyanChen/RSPrompter">Github</a>] [<a href="https://kyanchen.github.io/RSPrompter/">Page</a>] [<a href="https://huggingface.co/spaces/KyanChen/RSPrompter">Demo</a>]<br>
<div style="text-align: justify">
We consider designing an automated instance segmentation approach for remote sensing images based on the SAM foundation model, incorporating semantic category information with prompt learning. 
</div>
</div>

</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/2023_FunSR.png"><img src='image/2023_FunSR.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>Continuous Remote Sensing Image Super-Resolution based on Context Interaction in Implicit Function Space</b><br>
<i>IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2023</i><br>
<b>Keyan Chen</b>, Wenyuan Li, Sen Lei, Jianqi Chen, Xiaolong Jiang, Zhengxia Zou, and Zhenwei Shi<br>
[<a href="https://arxiv.org/abs/2302.08046">Arxiv</a>] [<a href="https://github.com/KyanChen/FunSR">Github</a>] [<a href="https://kyanchen.github.io/FunSR/">Page</a>] [<a href="https://huggingface.co/spaces/KyanChen/FunSR">Demo</a>]<br>
<div style="text-align: justify">
We propose a new super-resolution framework based on context interaction in implicit function space for learning continuous representations of remote sensing images, called FunSR, which consists of three main components: a functional representor, a functional interactor, and a functional parser.
</div>
</div>

</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/2023_ODSurvey.png"><img src='image/2023_ODSurvey.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>Object Detection in 20 Years: A Survey</b><br>
<i>Proceedings of the IEEE (P IEEE), 2023</i><br>
Zhengxia Zou, <b>Keyan Chen</b>, Zhenwei Shi, Yuhong Guo and Jieping Ye<br>
üèÜÔ∏è <b><font color="red">ESI Highly Cited Paper & ESI Hot Paper</font></b><br>
<b><font color="red">Popular Article of P IEEE</font></b><br>
[<a href="http://levir.buaa.edu.cn/publications/od_survey.pdf">PDF</a>] [<a href="https://github.com/KyanChen/ODSurvey">Github</a>] [<a href="https://kyanchen.github.io/ODSurvey/">Page</a>]<br>
<div style="text-align: justify">
This paper extensively reviews the fast-moving research field in the light of technical evolution, spanning over a quarter-century's time (from the 1990s to 2022). A number of topics have been covered, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed-up techniques, and the recent state-of-the-art detection methods.
</div>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/2022_RASNet.png"><img src='image/2022_RASNet.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>Resolution-agnostic Remote Sensing Scene Classification with Implicit Neural Representations</b><br>
<i>IEEE Geoscience and Remote Sensing Letters (GRSL), 2022</i><br>
<b>Keyan Chen</b>, Wenyuan Li, Jianqi Chen, Zhengxia Zou and Zhenwei Shi<br>
[<a href="http://levir.buaa.edu.cn/publications/RASNet.pdf">PDF</a>] [<a href="https://github.com/KyanChen/RASNet">Github</a>] [<a href="https://kyanchen.github.io/RASNet/">Page</a>]<br>
<div style="text-align: justify">
We propose a novel scene classification method with scale and resolution adaptation ability. Unlike previous CNNbased methods that make predictions based on rasterized image inputs, the proposed method converts the images as continuous functions with INRs optimization and then performs classification within the function space.
</div>

</div>
</div>


[//]: # (<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/GeCo.png"><img src='image/GeCo.png' alt="sym" width="100%"></a></div></div>)

[//]: # (<div class='paper-box-text' markdown="1">)

[//]: # ()
[//]: # (<b>Geographical Supervision Correction for Remote Sensing Representation Learning</b><br>)

[//]: # (<i>IEEE Transactions on Geoscience and Remote Sensing &#40;TGRS&#41;, 2022</i><br>)

[//]: # (Wenyuan Li, <b>Keyan Chen</b>, and Zhenwei Shi<br>)

[//]: # ([<a href="http://levir.buaa.edu.cn/publications/FINAL_VERSION.pdf">PDF</a>]<br>)

[//]: # (<div style="text-align: justify">)

[//]: # (We propose a Geographical supervision Correction method &#40;GeCo&#41; for remote sensing representation learning. Deviated geographical supervision generated by GLC products can be corrected adaptively using the correction matrix during network pre-training and joint optimization process is designed to simultaneously update the correction matrix and network parameters.)

[//]: # (</div>)

[//]: # ()
[//]: # (</div>)

[//]: # (</div>)



[//]: # (<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/DRENet.png"><img src='image/DRENet.png' alt="sym" width="100%"></a></div></div>)

[//]: # (<div class='paper-box-text' markdown="1">)

[//]: # ()
[//]: # (<b>A Degraded Reconstruction Enhancement-based Method for Tiny Ship Detection in Remote Sensing Images with A New Large-scale Dataset</b><br>)

[//]: # (<i>IEEE Transactions on Geoscience and Remote Sensing &#40;TGRS&#41;, 2022</i><br>)

[//]: # (Jianqi Chen, <b>Keyan Chen</b>, Hao Chen, Zhengxia Zou and Zhenwei Shi<br>)

[//]: # ([<a href="http://levir.buaa.edu.cn/publications/DRENet.pdf">PDF</a>] [<a href="https://github.com/WindVChen/DRENet">Github</a>] [<a href="https://github.com/windvchen/levir-ship">Dataset</a>]<br>)

[//]: # (<div style="text-align: justify">)

[//]: # (We propose a tiny ship detection method namely, Degraded Reconstruction Enhancement Network &#40;DRENet&#41;, for medium-resolution remote sensing images, and introduce Levir-Ship, which contains 3876 GF-1/GF-6 multi-spectral images and over 3K tiny ship instances.)

[//]: # (</div>)

[//]: # ()
[//]: # (</div>)

[//]: # (</div>)



[//]: # (<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/GeoKR.png"><img src='image/GeoKR.png' alt="sym" width="100%"></a></div></div>)

[//]: # (<div class='paper-box-text' markdown="1">)

[//]: # ()
[//]: # (<b>Geographical Knowledge-Driven Representation Learning for Remote Sensing Images</b><br>)

[//]: # (<i>IEEE Transactions on Geoscience and Remote Sensing &#40;TGRS&#41;, 2021</i><br>)

[//]: # (Wenyuan Li, <b>Keyan Chen</b>, Hao Chen and Zhenwei Shi<br>)

[//]: # ([<a href="http://levir.buaa.edu.cn/publications/Geographical_Knowledge-Driven.pdf">PDF</a>] [<a href="https://github.com/flyakon/Geographical-Knowledge-driven-Representaion-Learning">Github</a>]<br>)

[//]: # (<div style="text-align: justify">)

[//]: # ( We propose a Geographical Knowledge-driven Representation learning method for remote sensing images &#40;GeoKR&#41;, improving network performance and reduce the demand for annotated data. The global land cover products and geographical location associated with each remote sensing image are regarded as geographical knowledge to provide supervision for representation learning and network pre-training.)

[//]: # (</div>)

[//]: # ()
[//]: # (</div>)

[//]: # (</div>)


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/2021_STT.png"><img src='image/2021_STT.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>Building Extraction from Remote Sensing Images with Sparse Token Transformers</b><br>
<i>Remote Sensing, 2021</i><br>
<b>Keyan Chen</b>, Zhengxia Zou and Zhenwei Shi<br>
üèÜÔ∏è <b><font color="red">ESI Highly Cited Paper</font></b><br>
[<a href="https://www.mdpi.com/2072-4292/13/21/4441">PDF</a>] [<a href="https://github.com/KyanChen/STT">Github</a>] [<a href="https://kyanchen.github.io/STT/">Page</a>] [<a href="https://huggingface.co/spaces/KyanChen/BuildingExtraction">Demo</a>]<br>
<div style="text-align: justify">
We propose STT to explore the potential of using transformers for efficient building extraction. STT conducts an efficient dual-pathway transformer that learns the global semantic information in both their spatial and channel dimensions and achieves state-of-the-art accuracy on two building extraction benchmarks.
</div>

</div>
</div>
